{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-05T09:08:18.195137Z","iopub.execute_input":"2024-11-05T09:08:18.195606Z","iopub.status.idle":"2024-11-05T09:08:23.733641Z","shell.execute_reply.started":"2024-11-05T09:08:18.195559Z","shell.execute_reply":"2024-11-05T09:08:23.732360Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Load the model\nmodel_path = 'path/to/your/model.bin'  # Update with your model's path\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=2)  # Adjust num_labels if needed\nmodel.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\nmodel.eval()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the tokenizer (update with appropriate model name if needed)\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')  # Adjust with your model's tokenizer\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define test texts\ntest_texts = [\n    \"This is the first test sentence.\",\n    \"Here is another example sentence to try.\",\n    \"Let's see how this model performs!\"\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Tokenize and test the texts\nfor text in test_texts:\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    \n    # Get predictions (logits or softmax scores)\n    predictions = torch.softmax(outputs.logits, dim=-1)\n    print(f\"Text: {text}\")\n    print(f\"Predictions: {predictions}\")\n    print(\"\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}